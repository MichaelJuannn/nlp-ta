{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utf 8\n",
    "import re\n",
    "\n",
    "standardize_map = {\n",
    "    'ê': 'e', 'è': 'e', 'é': 'e', 'ë': 'e', 'Ê' : 'E',\n",
    "    'à': 'a', 'á': 'a', 'â': 'a', 'ä' : 'a', 'ă' :'a',\n",
    "    'î': 'i', 'ì': 'i', 'í': 'i', 'ï': 'i',\n",
    "    'ô': 'o', 'ò': 'o', 'ó': 'o', 'ö': 'o',\n",
    "    'ù': 'u', 'ú': 'u', 'û': 'u', 'ü': 'u'\n",
    "}\n",
    "\n",
    "def standardize_text(text):\n",
    "    for diacritic_char, standard_char in standardize_map.items():\n",
    "        text = text.replace(diacritic_char, standard_char)\n",
    "    return text\n",
    "\n",
    "with open('scrapped.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "\n",
    "standardized_text = standardize_text(text).lower()\n",
    "\n",
    "with open('standardized.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(standardized_text)\n",
    "    file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punc\n",
    "import re\n",
    "import string\n",
    "\n",
    "def remove_punctuation(text):\n",
    "\n",
    "    pattern = f\"[{re.escape(string.punctuation)}]\"\n",
    "    text_no_punctuation = re.sub(pattern, \"\", text)\n",
    "    \n",
    "    return text_no_punctuation\n",
    "\n",
    "v2_standardized_text = remove_punctuation(standardized_text)\n",
    "\n",
    "with open('standardized-2.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(v2_standardized_text)\n",
    "    file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove empty lines\n",
    "def remove_empty_lines(text):\n",
    "    lines = text.split(\"\\n\")\n",
    "    \n",
    "    non_empty_lines = [line for line in lines if line.strip()]\n",
    "    \n",
    "    cleaned_text = \"\\n\".join(non_empty_lines)\n",
    "    \n",
    "    return cleaned_text    \n",
    "\n",
    "\n",
    "v3_standardized_text = remove_empty_lines(v2_standardized_text)\n",
    "\n",
    "with open('standardized-3.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(v3_standardized_text)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total : 9365\n"
     ]
    }
   ],
   "source": [
    "# count\n",
    "def count_words(text):\n",
    "    words = text.split()\n",
    "    return len(words)\n",
    "\n",
    "word_count = count_words(v2_standardized_text)\n",
    "\n",
    "print(\"total :\", word_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah Kata Unik: 2885\n"
     ]
    }
   ],
   "source": [
    "# unique\n",
    "def get_unique_words(text):\n",
    "    words = text.split()\n",
    "    \n",
    "    unique_words = set(word.lower() for word in words)\n",
    "    \n",
    "    return list(unique_words)\n",
    "\n",
    "unique_words = get_unique_words(v3_standardized_text)\n",
    "length_of_unique_words = len(unique_words)\n",
    "\n",
    "print(\"Jumlah Kata Unik:\", len(unique_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['lingkup', 'pencarian', 'teks', 'dan', 'catatankakinya', 'teks', 'pencarian', '224', 'karakter', 'filter', 'pencarian', 'huruf', 'besarkecil', 'diakritik', 'serta', 'pungtuasi', 'diabaikan', 'karakter', 'dapat', 'digunakan', 'sebagai', 'pengganti', 'zero', 'atau', 'satu', 'huruf', 'sembarang', 'simbol', 'wildcard', 'dapat', 'digunakan', 'sebagai', 'pengganti', 'zero', 'atau', 'sejumlah', 'karakter', 'termasuk', 'spasi', 'mengakomodasi', 'variasi', 'ejaan', 'antara', 'lain', 'dj', 'j', 'tj', 'c', 'j', 'y', 'oe', 'u', 'd', 'dh', 't', 'th'], ['anggitanipun', 'dawud', 'magang', 'guru', 'ing', 'masaran']]\n"
     ]
    }
   ],
   "source": [
    "# [[],[]]\n",
    "def text_to_2d_list(text):\n",
    "\n",
    "    lines = text.split(\"\\n\")\n",
    "    \n",
    "    words_2d_list = [line.split() for line in lines if line.strip()]\n",
    "    \n",
    "    return words_2d_list\n",
    "\n",
    "\n",
    "words_2d_list = text_to_2d_list(v3_standardized_text)\n",
    "\n",
    "print(words_2d_list[:2])\n",
    "# v3_standardized_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_indices(unique_words):\n",
    "    word_to_idx = {}\n",
    "    idx_to_word = {}\n",
    "    for i, word in enumerate(unique_words):\n",
    "        word_to_idx[word] = i\n",
    "        idx_to_word[i] = word\n",
    "    return word_to_idx, idx_to_word\n",
    "\n",
    "word_to_idx, idx_to_word = build_indices(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anginggahi: 0\n",
      "grayang: 1\n",
      "ngeningaken: 2\n",
      "godha: 3\n",
      "jaler: 4\n",
      "lengenku: 5\n",
      "ngampet: 6\n",
      "pa: 7\n",
      "kasimpena: 8\n",
      "milar: 9\n"
     ]
    }
   ],
   "source": [
    "# word_to_idx\n",
    "\n",
    "for i, (word, idx) in enumerate(word_to_idx.items()):\n",
    "    print(f\"{word}: {idx}\")\n",
    "    if i == 9:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_corpus(corpus, word_to_idx):\n",
    "    sequences = []\n",
    "    \n",
    "    for line in corpus:\n",
    "        tokens = line\n",
    "        missing_tokens = [token for token in tokens if token not in word_to_idx]\n",
    "        \n",
    "        for token in missing_tokens:\n",
    "            word_to_idx[token] = len(word_to_idx)  \n",
    "            print(f\"Token '{token}' ditambahkan ke word_to_idx.\")\n",
    "        \n",
    "        for i in range(1, len(tokens)):\n",
    "            i_gram_sequence = tokens[:i+1]\n",
    "            i_gram_sequence_ids = [\n",
    "                word_to_idx[token] for token in i_gram_sequence\n",
    "            ]\n",
    "            sequences.append(i_gram_sequence_ids)\n",
    "    \n",
    "    return sequences\n",
    "\n",
    "sequences = prepare_corpus(words_2d_list, word_to_idx)\n",
    "max_sequence_len = max([len(x) for x in sequences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2394, 899]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cumeplong\n",
      "milanipun\n",
      "katamakake\n"
     ]
    }
   ],
   "source": [
    "print(idx_to_word[1647])\n",
    "print(idx_to_word[867])\n",
    "print(idx_to_word[1452])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9210"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sequence_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 16:03:58.518158: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-04 16:04:00.959563: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-04 16:04:02.312595: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733303043.862123   59553 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733303044.060097   59553 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-04 16:04:05.671664: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import utils\n",
    "import numpy as np\n",
    "\n",
    "def build_input_data(sequences, max_sequence_len, length_of_unique_words):\n",
    "    sequences = np.array(pad_sequences(sequences, maxlen = max_sequence_len, padding = 'pre'))\n",
    "    X = sequences[:,:-1]\n",
    "    y = sequences[:,-1]\n",
    "    y = utils.to_categorical(y, length_of_unique_words)\n",
    "    return X, y\n",
    "\n",
    "X, y = build_input_data(sequences, max_sequence_len, length_of_unique_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, LSTM, Dropout, Embedding, BatchNormalization, Bidirectional\n",
    "from keras.models import Sequential\n",
    "\n",
    "def create_model(max_sequence_len, length_of_unique_words):\n",
    "    model = Sequential([\n",
    "        Embedding(length_of_unique_words, 64, input_length= max_sequence_len-1),\n",
    "        Bidirectional(LSTM(64,return_sequences=True)),\n",
    "        Bidirectional(LSTM(64)),\n",
    "        BatchNormalization(),\n",
    "        Dense(128, activation=\"relu\"),\n",
    "        Dense(length_of_unique_words, activation=\"softmax\")\n",
    "    ])\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy'] \n",
    "    )\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_2 (\u001b[38;5;33mBidirectional\u001b[0m) │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_3 (\u001b[38;5;33mBidirectional\u001b[0m) │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization_1           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = create_model(max_sequence_len, length_of_unique_words)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9210"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-04 16:17:01.474013: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 106283400 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 2s/step - accuracy: 0.0361 - loss: 6.9255\n",
      "Epoch 2/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.0341 - loss: 6.7826\n",
      "Epoch 3/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 2s/step - accuracy: 0.0335 - loss: 6.6794\n",
      "Epoch 4/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.0337 - loss: 6.5740\n",
      "Epoch 5/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.0357 - loss: 6.4784\n",
      "Epoch 6/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 2s/step - accuracy: 0.0342 - loss: 6.3969\n",
      "Epoch 7/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - accuracy: 0.0324 - loss: 6.2890\n",
      "Epoch 8/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - accuracy: 0.0337 - loss: 6.1214\n",
      "Epoch 9/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 2s/step - accuracy: 0.0399 - loss: 5.9812\n",
      "Epoch 10/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 2s/step - accuracy: 0.0471 - loss: 5.8277\n",
      "Epoch 11/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 2s/step - accuracy: 0.0549 - loss: 5.6386\n",
      "Epoch 12/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 2s/step - accuracy: 0.0619 - loss: 5.5045\n",
      "Epoch 13/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - accuracy: 0.0702 - loss: 5.3159\n",
      "Epoch 14/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 2s/step - accuracy: 0.0863 - loss: 5.1029\n",
      "Epoch 15/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.0895 - loss: 4.9791\n",
      "Epoch 16/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2s/step - accuracy: 0.1053 - loss: 4.7883\n",
      "Epoch 17/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2s/step - accuracy: 0.1215 - loss: 4.5440\n",
      "Epoch 18/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 2s/step - accuracy: 0.1425 - loss: 4.2708\n",
      "Epoch 19/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 2s/step - accuracy: 0.1673 - loss: 4.0649\n",
      "Epoch 20/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 1s/step - accuracy: 0.1979 - loss: 3.8313\n",
      "Epoch 21/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 1s/step - accuracy: 0.2327 - loss: 3.5945\n",
      "Epoch 22/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 1s/step - accuracy: 0.2765 - loss: 3.3711\n",
      "Epoch 23/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 1s/step - accuracy: 0.3229 - loss: 3.1174\n",
      "Epoch 24/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 1s/step - accuracy: 0.3674 - loss: 2.8991\n",
      "Epoch 25/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 1s/step - accuracy: 0.4009 - loss: 2.7145\n",
      "Epoch 26/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 1s/step - accuracy: 0.4375 - loss: 2.5378\n",
      "Epoch 27/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 1s/step - accuracy: 0.4710 - loss: 2.3542\n",
      "Epoch 28/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 1s/step - accuracy: 0.5165 - loss: 2.1239\n",
      "Epoch 29/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 1s/step - accuracy: 0.5531 - loss: 1.9869\n",
      "Epoch 30/30\n",
      "\u001b[1m18/18\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 1s/step - accuracy: 0.5817 - loss: 1.8226\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x722d1d7633d0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X, y, batch_size = 512, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_text(seed_text, next_words, model, max_seq_len):\n",
    "#     for _ in range(next_words):\n",
    "#         sequences= prepare_corpus(words_2d_list[2], word_to_idx)\n",
    "#         sequences = pad_sequences([sequences[-1]], maxlen=max_seq_len-1, padding='pre')\n",
    "#         predicted = model.predict_classes(sequences, verbose=0)\n",
    "#         output_word = ''\n",
    "#         output_word = idx_to_word[predicted[0]]            \n",
    "#         seed_text = seed_text + \" \" + output_word\n",
    "        \n",
    "#     return seed_text.title()\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def generate_text(seed_text, next_words, model, max_seq_len, word_to_idx, idx_to_word):\n",
    "    for _ in range(next_words):\n",
    "        tokens = seed_text.split()\n",
    "        token_sequence = [word_to_idx.get(token, 0) for token in tokens] \n",
    "        \n",
    "        padded_sequence = pad_sequences([token_sequence], maxlen=max_seq_len - 1, padding='pre')\n",
    "        \n",
    "        predicted_index = np.argmax(model.predict(padded_sequence, verbose=0))\n",
    "        predicted_word = idx_to_word[predicted_index]\n",
    "        \n",
    "        seed_text += f\" {predicted_word}\"\n",
    "    \n",
    "    return seed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kacariyos ing jaman kina satunggiling wektu sang nata angandika dhateng anom ana ing pundhut putra putra priyantun jagi kawula abdi kawula kalilana andharaken ingkang prakawis ingkang badhe prakawis makaten punika minangka pratandha tementemening\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(\"kacariyos ing jaman\", 30, model, max_sequence_len, word_to_idx, idx_to_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_structure = model.to_json()\n",
    "with open(\"v1_text_generation.json\", \"w\") as json_file:\n",
    "    json_file.write(model_structure)\n",
    "model.save_weights(\"v1_text_generation.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
