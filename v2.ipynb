{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utf 8\n",
    "import re\n",
    "\n",
    "standardize_map = {\n",
    "    'ê': 'e', 'è': 'e', 'é': 'e', 'ë': 'e', 'Ê' : 'E',\n",
    "    'à': 'a', 'á': 'a', 'â': 'a', 'ä' : 'a', 'ă' :'a',\n",
    "    'î': 'i', 'ì': 'i', 'í': 'i', 'ï': 'i',\n",
    "    'ô': 'o', 'ò': 'o', 'ó': 'o', 'ö': 'o',\n",
    "    'ù': 'u', 'ú': 'u', 'û': 'u', 'ü': 'u'\n",
    "}\n",
    "\n",
    "def standardize_text(text):\n",
    "    for diacritic_char, standard_char in standardize_map.items():\n",
    "        text = text.replace(diacritic_char, standard_char)\n",
    "    return text\n",
    "\n",
    "with open('scrapped.txt', 'r', encoding='utf-8') as file:\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "\n",
    "standardized_text = standardize_text(text).lower()\n",
    "\n",
    "with open('standardized.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(standardized_text)\n",
    "    file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punc\n",
    "import re\n",
    "import string\n",
    "\n",
    "def remove_punctuation(text):\n",
    "\n",
    "    pattern = f\"[{re.escape(string.punctuation)}]\"\n",
    "    text_no_punctuation = re.sub(pattern, \"\", text)\n",
    "    \n",
    "    return text_no_punctuation\n",
    "\n",
    "v2_standardized_text = remove_punctuation(standardized_text)\n",
    "\n",
    "with open('standardized-2.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(v2_standardized_text)\n",
    "    file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove empty lines\n",
    "def remove_empty_lines(text):\n",
    "    lines = text.split(\"\\n\")\n",
    "    \n",
    "    non_empty_lines = [line for line in lines if line.strip()]\n",
    "    \n",
    "    cleaned_text = \"\\n\".join(non_empty_lines)\n",
    "    \n",
    "    return cleaned_text    \n",
    "\n",
    "\n",
    "v3_standardized_text = remove_empty_lines(v2_standardized_text)\n",
    "\n",
    "with open('standardized-3.txt', 'w', encoding='utf-8') as file:\n",
    "    file.write(v3_standardized_text)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total : 9365\n"
     ]
    }
   ],
   "source": [
    "# count\n",
    "def count_words(text):\n",
    "    words = text.split()\n",
    "    return len(words)\n",
    "\n",
    "word_count = count_words(v2_standardized_text)\n",
    "\n",
    "print(\"total :\", word_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah Kata Unik: 2885\n"
     ]
    }
   ],
   "source": [
    "# unique\n",
    "def get_unique_words(text):\n",
    "    words = text.split()\n",
    "    \n",
    "    unique_words = set(word.lower() for word in words)\n",
    "    \n",
    "    return list(unique_words)\n",
    "\n",
    "unique_words = get_unique_words(v3_standardized_text)\n",
    "length_of_unique_words = len(unique_words)\n",
    "\n",
    "print(\"Jumlah Kata Unik:\", len(unique_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['lingkup', 'pencarian', 'teks', 'dan', 'catatankakinya', 'teks', 'pencarian', '224', 'karakter', 'filter', 'pencarian', 'huruf', 'besarkecil', 'diakritik', 'serta', 'pungtuasi', 'diabaikan', 'karakter', 'dapat', 'digunakan', 'sebagai', 'pengganti', 'zero', 'atau', 'satu', 'huruf', 'sembarang', 'simbol', 'wildcard', 'dapat', 'digunakan', 'sebagai', 'pengganti', 'zero', 'atau', 'sejumlah', 'karakter', 'termasuk', 'spasi', 'mengakomodasi', 'variasi', 'ejaan', 'antara', 'lain', 'dj', 'j', 'tj', 'c', 'j', 'y', 'oe', 'u', 'd', 'dh', 't', 'th'], ['anggitanipun', 'dawud', 'magang', 'guru', 'ing', 'masaran']]\n"
     ]
    }
   ],
   "source": [
    "# [[],[]]\n",
    "def text_to_2d_list(text):\n",
    "\n",
    "    lines = text.split(\"\\n\")\n",
    "    \n",
    "    words_2d_list = [line.split() for line in lines if line.strip()]\n",
    "    \n",
    "    return words_2d_list\n",
    "\n",
    "\n",
    "words_2d_list = text_to_2d_list(v3_standardized_text)\n",
    "\n",
    "print(words_2d_list[:2])\n",
    "# v3_standardized_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_indices(unique_words):\n",
    "    word_to_idx = {}\n",
    "    idx_to_word = {}\n",
    "    for i, word in enumerate(unique_words):\n",
    "        word_to_idx[word] = i\n",
    "        idx_to_word[i] = word\n",
    "    return word_to_idx, idx_to_word\n",
    "\n",
    "word_to_idx, idx_to_word = build_indices(unique_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gulu: 0\n",
      "mbat: 1\n",
      "kathah: 2\n",
      "sadintenipun: 3\n",
      "wulu: 4\n",
      "sasampuning: 5\n",
      "lawas: 6\n",
      "dedamel: 7\n",
      "anggigat: 8\n",
      "mapan: 9\n"
     ]
    }
   ],
   "source": [
    "# word_to_idx\n",
    "\n",
    "for i, (word, idx) in enumerate(word_to_idx.items()):\n",
    "    print(f\"{word}: {idx}\")\n",
    "    if i == 9:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_corpus(corpus, word_to_idx):\n",
    "    sequences = []\n",
    "    \n",
    "    for line in corpus:\n",
    "        tokens = line\n",
    "        missing_tokens = [token for token in tokens if token not in word_to_idx]\n",
    "        \n",
    "        for token in missing_tokens:\n",
    "            word_to_idx[token] = len(word_to_idx)  \n",
    "            print(f\"Token '{token}' ditambahkan ke word_to_idx.\")\n",
    "        \n",
    "        for i in range(1, len(tokens)):\n",
    "            i_gram_sequence = tokens[:i+1]\n",
    "            i_gram_sequence_ids = [\n",
    "                word_to_idx[token] for token in i_gram_sequence\n",
    "            ]\n",
    "            sequences.append(i_gram_sequence_ids)\n",
    "    \n",
    "    return sequences\n",
    "\n",
    "sequences = prepare_corpus(words_2d_list, word_to_idx)\n",
    "max_sequence_len = max([len(x) for x in sequences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1579, 2550]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angengeti\n",
      "brangta\n",
      "tumuntena\n"
     ]
    }
   ],
   "source": [
    "print(idx_to_word[1647])\n",
    "print(idx_to_word[867])\n",
    "print(idx_to_word[1452])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9210"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sequence_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-07 16:21:24.453779: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-07 16:21:26.773306: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-12-07 16:21:28.102634: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1733563289.398829   37746 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1733563289.596037   37746 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-12-07 16:21:31.229842: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras import utils\n",
    "import numpy as np\n",
    "\n",
    "def build_input_data(sequences, max_sequence_len, length_of_unique_words):\n",
    "    sequences = np.array(pad_sequences(sequences, maxlen = max_sequence_len, padding = 'pre'))\n",
    "    X = sequences[:,:-1]\n",
    "    y = sequences[:,-1]\n",
    "    y = utils.to_categorical(y, length_of_unique_words)\n",
    "    return X, y\n",
    "\n",
    "X, y = build_input_data(sequences, max_sequence_len, length_of_unique_words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense, LSTM, Dropout, Embedding, BatchNormalization, Bidirectional\n",
    "from keras.models import Sequential\n",
    "\n",
    "def create_model(max_sequence_len, length_of_unique_words):\n",
    "    model = Sequential([\n",
    "        Embedding(length_of_unique_words, 64, input_length= max_sequence_len-1),\n",
    "        Bidirectional(LSTM(64,return_sequences=True)),\n",
    "        Bidirectional(LSTM(64)),\n",
    "        BatchNormalization(),\n",
    "        Dense(128, activation=\"relu\"),\n",
    "        Dense(length_of_unique_words, activation=\"softmax\")\n",
    "    ])\n",
    "    model.compile(\n",
    "        loss='categorical_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy'] \n",
    "    )\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/juanlul/repo/python-venv/NLP/lib/python3.10/site-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n",
      "W0000 00:00:1733563305.050032   37746 gpu_device.cc:2344] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>)   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Bidirectional</span>) │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional (\u001b[38;5;33mBidirectional\u001b[0m)   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ bidirectional_1 (\u001b[38;5;33mBidirectional\u001b[0m) │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ batch_normalization             │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = create_model(max_sequence_len, length_of_unique_words)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9210"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-07 16:21:45.905735: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 85026720 exceeds 10% of free system memory.\n",
      "2024-12-07 16:21:55.643494: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 38010880 exceeds 10% of free system memory.\n",
      "2024-12-07 16:21:55.659677: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 38010880 exceeds 10% of free system memory.\n",
      "2024-12-07 16:21:55.686112: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 38010880 exceeds 10% of free system memory.\n",
      "2024-12-07 16:21:56.982794: W external/local_xla/xla/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 38010880 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m15/15\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 2s/step - accuracy: 0.0031 - loss: 7.9144 - val_accuracy: 0.0206 - val_loss: 7.7842\n",
      "Epoch 2/75\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m75\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/repo/python-venv/NLP/lib/python3.10/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/repo/python-venv/NLP/lib/python3.10/site-packages/keras/src/backend/tensorflow/trainer.py:319\u001b[0m, in \u001b[0;36mTensorFlowTrainer.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39mcatch_stop_iteration():\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator\u001b[38;5;241m.\u001b[39menumerate_epoch():\n\u001b[0;32m--> 319\u001b[0m         \u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mon_train_batch_begin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m         logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_function(iterator)\n\u001b[1;32m    321\u001b[0m         callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_end(step, logs)\n",
      "File \u001b[0;32m~/repo/python-venv/NLP/lib/python3.10/site-packages/keras/src/callbacks/callback_list.py:153\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_begin\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    150\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n\u001b[1;32m    151\u001b[0m         callback\u001b[38;5;241m.\u001b[39mon_epoch_end(epoch, logs)\n\u001b[0;32m--> 153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mon_train_batch_begin\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch, logs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    154\u001b[0m     logs \u001b[38;5;241m=\u001b[39m logs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m callback \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train, validation_data=(X_test, y_test) ,batch_size = 512, epochs=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_text(seed_text, next_words, model, max_seq_len):\n",
    "#     for _ in range(next_words):\n",
    "#         sequences= prepare_corpus(words_2d_list[2], word_to_idx)\n",
    "#         sequences = pad_sequences([sequences[-1]], maxlen=max_seq_len-1, padding='pre')\n",
    "#         predicted = model.predict_classes(sequences, verbose=0)\n",
    "#         output_word = ''\n",
    "#         output_word = idx_to_word[predicted[0]]            \n",
    "#         seed_text = seed_text + \" \" + output_word\n",
    "        \n",
    "#     return seed_text.title()\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def generate_text(seed_text, next_words, model, max_seq_len, word_to_idx, idx_to_word):\n",
    "    for _ in range(next_words):\n",
    "        tokens = seed_text.split()\n",
    "        token_sequence = [word_to_idx.get(token, 0) for token in tokens] \n",
    "        \n",
    "        padded_sequence = pad_sequences([token_sequence], maxlen=max_seq_len - 1, padding='pre')\n",
    "        \n",
    "        predicted_index = np.argmax(model.predict(padded_sequence, verbose=0))\n",
    "        predicted_word = idx_to_word[predicted_index]\n",
    "        \n",
    "        seed_text += f\" {predicted_word}\"\n",
    "    \n",
    "    return seed_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kacariyos ing jaman kina wonten satunggiling warandha sampun sepuh anama bok randha sambega gegriya wonten ing padhekahan alit anama padhekahan sidhangmiring bawah ing sukawati sang nata utusan anglamar patih surasaning kapundhut ing dina\n"
     ]
    }
   ],
   "source": [
    "print(generate_text(\"kacariyos ing jaman\", 30, model, max_sequence_len, word_to_idx, idx_to_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_structure = model.to_json()\n",
    "with open(\"v1_text_generation.json\", \"w\") as json_file:\n",
    "    json_file.write(model_structure)\n",
    "model.save_weights(\"v1_text_generation.weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
